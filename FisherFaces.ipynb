{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to preface this project, this is purely for educational purposes. We are interested in seeing if linear algebra (fisherfaces) can guess someones race, based off a \"race space\". We do not mean to harm anyone, nor leave any groups out. Through this process, we have also seen biased datasets, where the dataset we are using is prominently white and male (people of European descent). \n",
    "\n",
    "Plan: Grab images from dataset for training & testing \n",
    "\n",
    "Hi everyone, just wanting to grab all the names and then keep like 1,000 of them because we think that 5,000 could lead to long computes\n",
    "\n",
    "In this project, we grabbed a dataset from Kaggle that had over 13,000 images (5,000+ distinct people) and we want to use fisherfaces to see if we can detect race based off of this dataset. \n",
    "\n",
    "To find their race, we will pass their name into chatgpt and have them be searched up online because these people will be relatively influential. and have chatgpt pass us back the json. \n",
    "**CHATGPT DOES NOT WORK**\n",
    "\n",
    "We will hand annotate 100 oeople each (300 people), keeping 250 for making the \"race space\", and 50 for testing. (We might handpick some pictures for testing because there is some spaces with a lack of data and we might not be able to see if the space even works) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of names in original dataset:  5760\n",
      "['Randall_Tobias', 'Tom_Moss', 'Ricardo_Monasterio', 'Ricky_Barnes', 'John_Engler', 'Anthony_Rackauckas', 'Matthew_Broderick', 'Frank_Solich', 'Daryl_Jones']\n",
      "length of random_names:  300\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random \n",
    "\n",
    "directory = '/Users/xavieryn/GitHub/QEAFinalProject/archive/lfw_funneled'\n",
    "\n",
    "arr = os.listdir(directory)\n",
    "#print(arr)\n",
    "print(\"length of names in original dataset: \", len(arr))\n",
    "\n",
    "random_names = random.sample(arr, 300)\n",
    "\n",
    "print(random_names[1:10])\n",
    "print(\"length of random_names: \", len(random_names) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we go with this dataset, then we need to grab the 300 images online and put it its own directory and then later split 250/50 for train and test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
